{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "We'll now go beyond the vanilla dense neural network we designed last time and look at some more advanced and specialized neural network architectures. Our first stop is the world of sequenctial data and Recurrent Neural Networks (RNNs). In this guide, we'll explore RNNs and their applications. Then, next time, we'll tackle some of the limitations of RNNs and how to address them.\n",
    "\n",
    "RNNs are a class of neural networks that are tailored for sequential data such as text, audio and time series data. To appreciate the value of RNNs for such data, it's important to first understand the limitations of the standard dense neural network.\n",
    "\n",
    "## Limitations of Dense Neural Networks\n",
    "\n",
    "While dense neural networks are powerful and versatile, they come with certain shortcomings, especially when handling sequential data:\n",
    "\n",
    "1. **No Sense of Order**: Dense neural networks treat inputs as independent entities. For instance, if we feed a sentence to a dense neural network, it will process each word in isolation, without any sense of order or context. This limitation is especially relevant for sequential data, where the order of elements is often crucial for their interpretation.\n",
    "    - For example, a dense neural network might treat the sentences \"Man eats shark\" and \"Shark eats man\" as equivalent, even though they have very different meanings.\n",
    "2. **Fixed Input and Output Sizes**: Dense networks require a predetermined size for their input and output layers. This can be limiting for tasks where the length of the input sequence might vary, such as processing sentences of different lengths in natural language tasks.\n",
    "3. **Parameter Efficiency**: For large input sizes, the number of parameters in a dense neural network can grow exponentially, leading to overfitting and increased computational demands.\n",
    "\n",
    "## In Comes Recurrent Neural Networks\n",
    "\n",
    "RNNs where introduced to to address these challenges. Unlike dense neural networks that require fixed input and output sizes, RNNs are inherently designed to process sequences of variable lengths. This flexibility arises from their unique architecture that loops over the sequence, handling just one element at a time (RNNs should really be tought of as a looping neural network). This looping design means that whether a sentence has five words or fifty, RNNs can process it without the need for reshaping or truncating the input.\n",
    "\n",
    "Moreover, as the RNN process sequences step-by-step, it maintains a hidden state that carries information from prior steps. This \"memory\" ensures that context is preserved. For instance, in a sentence, the meaning of a word can often be influenced by preceding words. RNNs, by virtue of this internal memory, can capture such dependencies, ensuring that every input is understood not just in isolation, but in the context of its preceding elements..\n",
    "\n",
    "First, we'll import the necessary libraries. Next, we'll explore the the `ValueArray` class and its purpose. After that, we'll dive into RNN architectures and conclude by building and training a RNN from the ground up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To allow importing modules from the dlafs directory, can be ignored\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path of the 'src' directory\n",
    "src_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "# Add 'src' directory to the Python path\n",
    "if src_dir not in sys.path:\n",
    "    sys.path.append(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import dlafs\n",
    "from dlafs import Value, ValueArray\n",
    "from dlafs.nn import BaseNeuron, Module, Layer\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ValueArray\n",
    "\n",
    "Moving on forward, we'll be working with neural network architectures that requires multi-dimensional arrays (i.e. multidimensional lists of numbers). Since this is a pure python neural networks from scratch guide, we'll have to use our own implementation of multi-dimensional arrays instead of other libraries such as `numpy`.\n",
    "\n",
    "To that end, we'll use the `ValueArray` class to represent multi-dimensional arrays. The `ValueArray` class is implemented using\n",
    "standard python lists and can only hold `Value` objects. The API is quite simple since it doesn't implement any mathematical operations. Instead, it's designed to be used as a container for data, and only provides methods for getting/setting values and converting to/from lists and numpy arrays.\n",
    "\n",
    "We'll use the rest of this section to explore the API of the `ValueArray` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueArray(\n",
      "    [[0, 0],\n",
      "     [0, 0],\n",
      "     [0, 0]],\n",
      "    label='test'\n",
      ")\n",
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "# Creating a ValueArray of zeros\n",
    "arr = ValueArray.zeros(shape=(3, 2), label='test')\n",
    "print(arr)\n",
    "print(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(0, label='test_0_0')\n",
      "ValueArray(\n",
      "    [0, 0]\n",
      ")\n",
      "ValueArray(\n",
      "    [0, 0, 0]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Getting values\n",
    "print(arr[0, 0])  # Returns a Value object\n",
    "print(arr[0,])\n",
    "print(arr[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ValueArray(\n",
       "    [[1, 0],\n",
       "     [0, 0],\n",
       "     [3, 3]],\n",
       "    label='test'\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting values\n",
    "arr[0, 0] = 1  # Converts to a Value object\n",
    "arr[2, :] = [3, 3]\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueArray(\n",
      "    [[0, 1],\n",
      "     [2, 3]]\n",
      ")\n",
      "[[0 1]\n",
      " [2 3]]\n"
     ]
    }
   ],
   "source": [
    "# To and from numpy arrays\n",
    "np_arr = np.arange(24).reshape(3, 2, 2, 2)\n",
    "v_arr = ValueArray.from_numpy(np_arr)\n",
    "print(v_arr[0, 0, :, :])\n",
    "print(v_arr[0, 0, :, :].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueArray(\n",
      "    [[1.41, 2.22],\n",
      "     [ 3.3,  4.4],\n",
      "     [ 5.5,  6.6]]\n",
      ")\n",
      "[[1.41, 2.22], [3.3, 4.4], [5.5, 6.6]]\n",
      "[[Value(1.41), Value(2.22)], [Value(3.3), Value(4.4)], [Value(5.5), Value(6.6)]]\n"
     ]
    }
   ],
   "source": [
    "# To and from lists\n",
    "py_list = [[1.41, 2.22], [3.3, 4.4], [5.5, 6.6]]\n",
    "v_arr = ValueArray(py_list)\n",
    "\n",
    "print(v_arr)\n",
    "\n",
    "print(v_arr.to_list())  # Returns a nested list of floats or ints\n",
    "print(v_arr.values)  # Returns a nested list of Value objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueArray(\n",
      "    [[ -0.14409, -0.172904],\n",
      "     [-0.111316,  0.701984],\n",
      "     [-0.127588, -1.497353]]\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ValueArray(\n",
       "    [[4.460898, 0.434694],\n",
       "     [2.109609, 0.148986],\n",
       "     [ 1.09319, 2.526776]]\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a ValueArray of random values\n",
    "# sampled from a normal dist with mean 0 and std 1.\n",
    "print(ValueArray.random_normal(shape=(3, 2)))\n",
    "\n",
    "# Creating a ValueArray of random values\n",
    "# sampled from a uniform dist between 0 and 5.\n",
    "ValueArray.random_uniform(shape=(3, 2), low=0, high=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The RNN Neuron\n",
    "\n",
    "The RNN neuron is the basic building block of RNNs. It's similar to the standard neuron we've seen before, but with a few key differences. Let's start by looking at the standard neuron again:\n",
    "\n",
    "<img src=\"img/neuron_model.png\" alt=\"Standard Neuron\" width=\"600\"/>\n",
    "\n",
    "The standard neuron takes a list of inputs $x_i$, multiplies it with the corresponding weights $w_i$, sums them together, and adds a bias $b$ to the result. Lastly, it passes the weighted sum through an activation function $f$ to produce the output $a$.\n",
    "\n",
    "$$\n",
    "a = f(\\sum_{i=1}^{n} w_i x_i + b)\n",
    "$$\n",
    "\n",
    "A RNN neuron is illustrated below (Note that superscripts denote the time step or what inputs the weights are multiplied with):\n",
    "\n",
    "<img src=\"img/recurrent_neuron_model.png\" alt=\"RNN Neuron\" width=\"600\"/>\n",
    "\n",
    "As you can see, the RNN neuron is similar, but it has a key difference: it has a hidden state $a^{\\braket{t}}$ that is passed from one timestep (denoted $\\braket{t}$) to the next. \n",
    "\n",
    "This hidden state is computed by combining the current input $x^{\\braket{t}}$ and the previous hidden state $a^{\\braket{t-1}}$. The inputs are multiplied by their corresponding weights $w^x$ and $w^a$ and summed together (+ a bias $b$), and then passed through an activation function to compute the hidden state $a^{\\braket{t}}$. The hidden state is then passed as an input to the next timestep $t+1$.\n",
    "\n",
    "This gives us the following equation for the hidden state:\n",
    "\n",
    "$$\n",
    "a^{\\braket{t}}_i = f(\\sum_{i=1}^{n^x} w^x_i x^{\\braket{t}}_i + \\sum_{j=1}^{n^a}w^a_j a^{\\braket{t-1}}_j + b)\n",
    "$$\n",
    "\n",
    "Where the activation function $f$ is usually $tanh$.\n",
    "\n",
    "The formula above outputs the hidden state for a single neuron $a^{\\braket{t}}_i$. However, it takes $n^a$ inputs from the previous hidden state, meaning that we'll need $n^a$ neurons to compute the new hidden state. We'll call this collection of neurons the recurrent layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Basic RNN Architecture\n",
    "\n",
    "A RNN uses the same recurrent layer in each timestep, passing the hidden state from one timestep to the next. This allows the RNN to loop through a (arbitrary-length) sequence of inputs one at a time. The hidden state acts as the neuron's memory, allowing it to retain information from previous inputs. The looping design is illustrated below:\n",
    "\n",
    "<img src=\"img/rnn_simplified.png\" alt=\"RNN Architecture\" width=\"800\"/>\n",
    "\n",
    "One thing we haven't mentioned yet is how the output $\\hat{y}^{\\braket{t}}$ is computed. We can simply compute $\\hat{y}^{\\braket{t}}$ by passing the hidden state $a^{\\braket{t}}$ as the input to a standard neuron.\n",
    "\n",
    "Lastly, it should be noted that the first timestep $\\braket{t}=0$ is a special case. At this timestep, there is no previous hidden state $a^{\\braket{t-1}}$ to use. Instead, the hidden state $a^{\\braket{-1}}$ is initialized to a list of zeros.\n",
    "\n",
    "### Different Types of RNNs\n",
    "\n",
    "While the image above gives a general idea of RNNs, it's essential to recognize that not all RNNs are identical. The output $\\hat{y}$​ might not be computed at each timestep $\\braket{t}$. This is because there are different RNNs designs, each suited for different tasks. \n",
    "\n",
    "<img src=\"img/rnn_types.jpeg\" alt=\"RNN Types\" width=\"900\"/>\n",
    "\n",
    "**One-to-One**: This is the standard neural network architecture, where one input is processed to produce one output. It doesn't utilize sequences and is more akin to traditional feed-forward neural networks.\n",
    "\n",
    "**One-to-Many**: This type of RNN is designed to take one input and produce a sequence of outputs. It's commonly used in tasks like image captioning, where an image (single input) is used to generate a sentence (sequence of words).\n",
    "\n",
    "**Many-to-One**: Here, a sequence of inputs produces a single output. This architecture is typically employed in tasks like sentiment analysis, where a sequence of words in a sentence is used to determine a single sentiment score or category.\n",
    "\n",
    "**Many-to-Many(left)**: This design processes a sequence of inputs to produce a sequence of outputs where the lengths of input and output sequences are the same. It's often used in tasks like part-of-speech tagging, where each word in an input sentence is tagged with its corresponding part of speech.\n",
    "\n",
    "**Many-to-Many (right)**: Also known as \"sequence-to-sequence\", this type of RNN takes a sequence of inputs and produces a potentially different-length sequence of outputs. This architecture is crucial for tasks like machine translation, where an input sentence in one language is translated to an output sentence in another language, and the lengths of the two sentences may differ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with implementing the Recurrent Neuron. It takes the number of inputs and the size of the hidden state (number of recurrent neurons) as parameters. It also takes an activation function as a parameter, which defaults to `tanh` if none is provided.\n",
    "(The last parameter `neuron_id` can be ignored since it's only used for labeling `Value` objects)\n",
    "\n",
    "The init method initialized the weights to random numbers, like in the standard neuron. While the call method outputs the new hidden state as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN-Neuron(2)\n"
     ]
    }
   ],
   "source": [
    "class RecurrentNeuron(BaseNeuron):\n",
    "\n",
    "    def __init__(self, num_inputs, hidden_size, activation='tanh', neuron_id=None):\n",
    "        if neuron_id is not None:\n",
    "            neuron_id = f'_{neuron_id}'\n",
    "        # Initialize the weights and bias\n",
    "        self.wx = ValueArray.random_normal(shape=(num_inputs, ), label=f'wx{neuron_id}',\n",
    "                                           mean=0, std=2/(num_inputs+hidden_size))\n",
    "        self.wa = ValueArray.random_normal(shape=(hidden_size, ), label=f'wa{neuron_id}',\n",
    "                                           mean=0, std=1/(hidden_size))\n",
    "        self.ba = Value(0, label='ba')\n",
    "        self._activation = activation\n",
    "\n",
    "    def __call__(self, x, a):\n",
    "        \"\"\"The forward pass of a single neuron\"\"\"\n",
    "        if len(x) != self.wx.shape[0]:\n",
    "            raise ValueError(f'Expected {self.wx.shape[0]} inputs, got {len(x)}')\n",
    "        if not len(a) == self.wa.shape[0]:\n",
    "            raise ValueError(f'Expected {self.wa.shape[0]} hidden inputs, got {len(a)}')\n",
    "        a = ValueArray(a, label='a')\n",
    "\n",
    "        zx = sum(wx_i * x_i for wx_i, x_i in zip(self.wx, x))\n",
    "        za = sum(wa_i * a_i for wa_i, a_i in zip(self.wa, a))\n",
    "        z = zx + za + self.ba\n",
    "\n",
    "        out = self.activation(z)\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"Return the weights and bias as a list\"\"\"\n",
    "        return self.wx.values + self.wa.values + [self.ba]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'RNN-Neuron({self.wx.shape[0]})'\n",
    "\n",
    "\n",
    "neuron = RecurrentNeuron(num_inputs=2, hidden_size=1, activation='tanh')\n",
    "print(neuron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this single recurrent neuron to make predictions on a sequence of inputs. However, the hidden state can only be of size one (i.e. a single neuron)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(0.526363)\n",
      "Value(0.193986)\n"
     ]
    }
   ],
   "source": [
    "inputs = [[Value(1, label='x_1'), Value(2, label='x_2')], [Value(1, label='x_1'), Value(2, label='x_2')]]\n",
    "a_0 = [0,]\n",
    "a_1 = neuron(inputs[0], a_0)\n",
    "print(a_1)\n",
    "a_2 = neuron(inputs[1], [a_1,])\n",
    "print(a_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To increase the size of the hidden state, we'll need to use multiple recurrent neurons organized in a layer. We'll call this layer the `RecurrentLayer`. \n",
    "\n",
    "The init method takes the same arguments as the `RecurrentNeuron` (number of inputs, hidden state size, activation) and initializes a list of `RecurrentNeuron` objects.\n",
    "\n",
    "The call method takes a sequence of inputs in the shape *(sequence length, number of inputs)* and outputs a sequence of hidden states in the shape *(sequence length, hidden state size)*. It computes the hidden states by initializing the first hidden state to a list of zeros, and then looping through the inputs, passing the current input and the previous hidden state to each neuron in the layer. It then appends the new hidden state to a list of hidden states and returns it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentLayer(Module):\n",
    "\n",
    "    def __init__(self, num_inputs, hidden_size, activation='tanh'):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.neurons = [RecurrentNeuron(num_inputs, hidden_size, activation, neuron_id=i) for i in range(hidden_size)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = ValueArray(x)\n",
    "        # Check that the number of inputs equals the number of weights\n",
    "        if not x.shape[1] == self.neurons[0].wx.shape[0]:\n",
    "            raise ValueError(f'Expected {self.neurons[0].wx.shape[0]} inputs, got {x.shape[1]}')\n",
    "\n",
    "        a_t = ValueArray.zeros(shape=(self.hidden_size,), label='a_t')  # Initialize hidden state to zeros\n",
    "        a = []\n",
    "        for x_t in x:\n",
    "            a_t = [n(x_t, a_t) for n in self.neurons]\n",
    "            a.append(a_t)\n",
    "        return ValueArray(a)\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"Return the weights and bias as a list\"\"\"\n",
    "        return [p for n in self.neurons for p in n.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"RecurrentLayer({self.neurons})\"\n",
    "\n",
    "layer = RecurrentLayer(num_inputs=3, hidden_size=4, activation='tanh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now only missing one piece of the puzzle: the output neuron. We can create a RNN by combining the `RecurrentLayer` with layers of standard neurons. We'll call this combination the `RecurrentNN` class. Note that we can't use the `VanillaNN` class we created last time since it's not designed to handle sequences of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentNN(Module):\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = ValueArray(x)\n",
    "        for layer in self.layers:\n",
    "            if x.dim() > 1 and not isinstance(layer, RecurrentLayer):\n",
    "                new_x = []\n",
    "                for x_t in x:\n",
    "                    new_x.append(layer(x_t))\n",
    "                x = ValueArray(new_x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        layers_str = ',\\n  '.join([str(layer) for layer in self.layers])\n",
    "        return f\"RecurrentNN([\\n  {layers_str}\\n])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecurrentNN([\n",
      "  RecurrentLayer([RNN-Neuron(2), RNN-Neuron(2), RNN-Neuron(2), RNN-Neuron(2), RNN-Neuron(2)]),\n",
      "  Layer([SigmoidNeuron(5)])\n",
      "])\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "    RecurrentLayer(num_inputs=2, hidden_size=5, activation='tanh'),\n",
    "    Layer(5, 1, activation='sigmoid')\n",
    "]\n",
    "rnn = RecurrentNN(layers)\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you have it! We've now implemented a basic RNN from scratch. We can use it to make predictions on sequences of inputs of arbitrary length. Let's put it to the test on a simple task. \n",
    "\n",
    "The task is to predict classify if the sum of the inputs is increasing at every timestep. Where the input for each timestep is a list of two numbers. For instance, the sequence `[[1, 2], [3, 4]]` should be classified as `True` (or 1) while `[[1, 2], [0, 2]]` should be classified as `False` (or 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(0.628636)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [\n",
    "    [(1, 3), (4, 2), [5, 7], [-2, 15]],\n",
    "    [(4, 2), (1, 3)],\n",
    "    [(5, 7), (2, 4), (7, 7)],\n",
    "    [(-1, 0), (2, 1)],\n",
    "    [(0, -1), (1, 2), [2, 0]],\n",
    "    [(-10, 11), (2, 0), (1.5, 1), (11, -7), (-4, 10)]\n",
    "]\n",
    "labels = ValueArray([[1], [0], [0], [1], [0], [1]])\n",
    "output = rnn(inputs[0])[-1]\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll define the training loop. We'll use cross entropy as loss function (as described in the last notebook). The RNN is many-to-one, so we'll only use the last output to make the prediction. The rest of the code is similar to the training loop we used last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss=0.818\n",
      "Epoch 10, loss=0.308\n",
      "Epoch 20, loss=0.150\n",
      "Epoch 30, loss=0.083\n",
      "Epoch 40, loss=0.054\n",
      "Epoch 50, loss=0.040\n",
      "Epoch 60, loss=0.031\n",
      "Epoch 70, loss=0.025\n",
      "Epoch 80, loss=0.022\n",
      "Epoch 90, loss=0.019\n",
      "Epoch 100, loss=0.016\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(101):\n",
    "    outputs = [[rnn(x)[-1]] for x in inputs]  # Many-to-one.\n",
    "    loss = dlafs.loss.cross_entropy(labels, outputs)\n",
    "    labels.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in rnn.parameters():\n",
    "        param.data -= 3e-1 * param.grad\n",
    "    rnn.zero_grad()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, loss={loss.data:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 0.986, label: 1\n",
      "Prediction: 0.015, label: 0\n",
      "Prediction: 0.009, label: 0\n",
      "Prediction: 0.974, label: 1\n",
      "Prediction: 0.020, label: 0\n",
      "Prediction: 0.989, label: 1\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "outputs = [rnn(x)[-1] for x in inputs]\n",
    "for output, label in zip(outputs, labels):\n",
    "    print(f'Prediction: {output.data:.3f}, label: {label.values[0].data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RNN is able to fit the training data quite good, but since the dataset is so small, it will not be able to generalize to new samples. At least we know that it's working as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(0.997981), Value(0.074818), Value(0.964868)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = [\n",
    "    [[2, 5], [-30, -1]],\n",
    "    [[1, 0], [0, 1], [1, 0], [0, 1]],\n",
    "    [[0, 0], [1, 2], [7, 30]],\n",
    "    ]\n",
    "[rnn(x)[-1] for x in x_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we end this notebook I'll show how we can create a one-to-many RNN. There are several ways to achieve this using our current architecture:\n",
    "\n",
    "1. We can simply pass zeros as the input for all timesteps after the first one.\n",
    "2. We can repeatedly pass the same input $x^{\\braket{0}}$ for all timesteps.\n",
    "3. We can pass the output $\\hat{y}^{\\braket{t}}$ (not $a^{\\braket{t}}$) as the input for the next timestep.\n",
    "\n",
    "I'll showcase this last approach since it's the most common. We'll create a new dataset where the input is a simple integer and the outputs is the next 5 integers. For instance, the input `10` should produce the output `[11, 12, 13, 14, 15]`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ValueArray(\n",
       "    [[[0]],\n",
       "\n",
       "     [[1]],\n",
       "\n",
       "     [[2]],\n",
       "\n",
       "     [[3]],\n",
       "\n",
       "     [[4]]]\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(0, 100, 1)\n",
    "x = x.reshape(-1, 1, 1)  # (num_samples, seq_length, input_size) = (100, 1, 1)\n",
    "x = ValueArray.from_numpy(x)\n",
    "x[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ValueArray(\n",
       "    [[1, 2, 3, 4, 5],\n",
       "     [2, 3, 4, 5, 6],\n",
       "     [3, 4, 5, 6, 7],\n",
       "     [4, 5, 6, 7, 8],\n",
       "     [5, 6, 7, 8, 9]]\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [np.arange(i+1, i+6).tolist() for i in range(0, 100)]\n",
    "y = ValueArray(y)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a simple RNN which takes 1 input and has a hidden size of 4. The output layer will be a linear layer with 1 output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    RecurrentLayer(num_inputs=1, hidden_size=4, activation='linear'),\n",
    "    Layer(4, 1, activation='linear')\n",
    "]\n",
    "rnn = RecurrentNN(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a function to generate predictions, where each prediction is fed as the input to the next timestep until we've reached `n_steps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ValueArray(\n",
       "    [-0.092553, -0.079858, -0.081599, -0.081361, -0.081393]\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one_to_many_predict(model, x_0, n_steps):\n",
    "    outputs = []\n",
    "    x_i = x_0\n",
    "    for _ in range(n_steps):\n",
    "        y_hat_i = model(x_i)\n",
    "        outputs.append(y_hat_i[-1])\n",
    "        x_i = ValueArray([y_hat_i.values])\n",
    "    return ValueArray(outputs)\n",
    "\n",
    "one_to_many_predict(rnn, x[0], 5)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can define the training loop, which looks similar to the one we used previously, except that we're using the `one_to_many_predict` function to generate the predictions, and that we average the loss over all timesteps using MSE as a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss=3773.591\n",
      "Epoch 10, loss=3653.840\n",
      "Epoch 20, loss=3514.899\n",
      "Epoch 30, loss=3322.788\n",
      "Epoch 40, loss=2957.363\n",
      "Epoch 50, loss=1276.207\n",
      "Epoch 60, loss=2.771\n",
      "Epoch 70, loss=2.765\n",
      "Epoch 80, loss=2.760\n",
      "Epoch 90, loss=2.754\n",
      "Epoch 100, loss=2.748\n"
     ]
    }
   ],
   "source": [
    "# NB: takes a while to run\n",
    "for epoch in range(101):\n",
    "    outputs = ValueArray([one_to_many_predict(rnn, x_i, 5) for x_i in x])\n",
    "    # Average loss over the 5 outputs per sample\n",
    "    loss = sum(dlafs.loss.mse(y[:, i], outputs[:, i]) for i in range(5)) / 5\n",
    "    labels.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in rnn.parameters():\n",
    "        param.data -= 7e-6 * param.grad\n",
    "    rnn.zero_grad()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, loss={loss.data:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same function can be used to generate predictions of any length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ValueArray(\n",
       "    [[101.470042, 102.961581, 104.474929, 106.010408, 107.568339, 109.149051, 110.752878, 112.380158],\n",
       "     [102.484665,  103.99104, 105.519442, 107.070194, 108.643623, 110.240059, 111.859839, 113.503306],\n",
       "     [103.499288, 105.020499, 106.563955, 108.129981, 109.718906, 111.331066, 112.966801, 114.626454],\n",
       "     [104.513911, 106.049959, 107.608468, 109.189768,  110.79419, 112.422074, 114.073762, 115.749602],\n",
       "     [105.528533, 107.079418, 108.652981, 110.249555, 111.869474, 113.513081, 115.180723,  116.87275]]\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = np.arange(100, 105).reshape(-1, 1, 1)\n",
    "test_x = ValueArray.from_numpy(test_x)\n",
    "preds = ValueArray([one_to_many_predict(rnn, x_i, 8) for x_i in test_x])\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've created a recurrent neural network from scratch using only pure Python. Next up, we'll look at some of the limitations of RNNs, namely vanishing and exploding gradients, and how to address them using LSTMs.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
